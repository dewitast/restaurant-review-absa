{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('review_text.csv',encoding='utf8')\n",
    "docs = df['reviews'].values\n",
    "data = []\n",
    "for doc in docs:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentence = []\n",
    "    for item in pos:\n",
    "        words = {}\n",
    "        words['token'] = item[0]\n",
    "        words['pos'] = item[1]\n",
    "        words['label'] = 'O' #Default BIO tag\n",
    "        sentence.append(words)\n",
    "    data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "a = [2,3,4,5]\n",
    "a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('reviews-aspect-0-299.json', 'w') as outfile:\n",
    "    json.dump(data[:300], outfile)\n",
    "with open('reviews-aspect-300-599.json', 'w') as outfile:\n",
    "    json.dump(data[300:600], outfile)\n",
    "with open('reviews-aspect-600.json', 'w') as outfile:\n",
    "    json.dump(data[600:], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Words with BIO Notation Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[('It', 'PRP', 'O'),\n",
       "  ('is', 'VBZ', 'O'),\n",
       "  ('very', 'RB', 'O'),\n",
       "  ('gud', 'JJ', 'O'),\n",
       "  ('place', 'NN', 'B'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('get', 'VB', 'O'),\n",
       "  ('food', 'NN', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('food', 'NN', 'B'),\n",
       "  ('is', 'VBZ', 'O'),\n",
       "  ('too', 'RB', 'O'),\n",
       "  ('gud', 'JJ', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('mouth', 'JJ', 'O'),\n",
       "  ('water', 'NN', 'O'),\n",
       "  ('fascinating', 'VBG', 'O'),\n",
       "  ('quality', 'NN', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('quantity', 'NN', 'O'),\n",
       "  ('that', 'IN', 'O'),\n",
       "  ('u', 'JJ', 'O'),\n",
       "  ('ever', 'RB', 'O'),\n",
       "  ('going', 'VBG', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('get', 'VB', 'O'),\n",
       "  ('awesome', 'JJ', 'O'),\n",
       "  ('food', 'NN', 'O'),\n",
       "  ('really', 'RB', 'O'),\n",
       "  ('loved', 'VBD', 'O'),\n",
       "  ('it', 'PRP', 'O')],\n",
       " [('Great', 'NNP', 'O'),\n",
       "  ('food', 'NN', 'B'),\n",
       "  (',', ',', 'O'),\n",
       "  ('indoor', 'NN', 'O'),\n",
       "  ('and', 'CC', 'O'),\n",
       "  ('outdoor', 'NN', 'O'),\n",
       "  ('seating', 'NN', 'O')],\n",
       " [('Loved', 'VBN', 'O'),\n",
       "  ('all', 'DT', 'O'),\n",
       "  ('our', 'PRP$', 'O'),\n",
       "  ('food', 'NN', 'B'),\n",
       "  (',', ',', 'O'),\n",
       "  ('licensed', 'VBD', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('fast', 'RB', 'O'),\n",
       "  (',', ',', 'O'),\n",
       "  ('efficient', 'JJ', 'O'),\n",
       "  ('service', 'NN', 'B')]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('reviews-aspect-label.json') as f:\n",
    "    labeled = json.load(f)\n",
    "\n",
    "display(type(labeled))\n",
    "data = []\n",
    "for sentence in labeled:\n",
    "    sen = []\n",
    "    for item in sentence:\n",
    "        tup = (item['token'],item['pos'],item['label'])\n",
    "        sen.append(tup)\n",
    "    data.append(sen)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    postag = doc[i][1]\n",
    "\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'postag=' + postag\n",
    "    ]\n",
    "\n",
    "    # Fitur untuk kata non awal kalimat\n",
    "    if i > 0:\n",
    "        word1 = doc[i-1][0]\n",
    "        postag1 = doc[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS') # Tanda awal kalimat\n",
    "\n",
    "    # Fitur untuk kata non akhir kalimat\n",
    "    if i < len(doc)-1:\n",
    "        word1 = doc[i+1][0]\n",
    "        postag1 = doc[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS') #tanda akhir kalimat\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bias', 'word.lower=all', 'postag=DT', '-1:word.lower=loved', '-1:postag=VBN', '+1:word.lower=our', '+1:postag=PRP$']\n"
     ]
    }
   ],
   "source": [
    "feat = word2features(train[2],1)\n",
    "print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def extract_features(doc):\n",
    "    return [word2features(doc, i) for i in range(len(doc))]\n",
    "\n",
    "def get_labels(doc):\n",
    "    return [label for (token, postag, label) in doc]\n",
    "\n",
    "X = [extract_features(doc) for doc in data]\n",
    "y = [get_labels(doc) for doc in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['bias',\n",
       "   'word.lower=great',\n",
       "   'postag=NNP',\n",
       "   'BOS',\n",
       "   '+1:word.lower=food',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=food',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=great',\n",
       "   '-1:postag=NNP',\n",
       "   '+1:word.lower=,',\n",
       "   '+1:postag=,'],\n",
       "  ['bias',\n",
       "   'word.lower=,',\n",
       "   'postag=,',\n",
       "   '-1:word.lower=food',\n",
       "   '-1:postag=NN',\n",
       "   '+1:word.lower=indoor',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=indoor',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=,',\n",
       "   '-1:postag=,',\n",
       "   '+1:word.lower=and',\n",
       "   '+1:postag=CC'],\n",
       "  ['bias',\n",
       "   'word.lower=and',\n",
       "   'postag=CC',\n",
       "   '-1:word.lower=indoor',\n",
       "   '-1:postag=NN',\n",
       "   '+1:word.lower=outdoor',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=outdoor',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=and',\n",
       "   '-1:postag=CC',\n",
       "   '+1:word.lower=seating',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=seating',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=outdoor',\n",
       "   '-1:postag=NN',\n",
       "   'EOS']],\n",
       " [['bias',\n",
       "   'word.lower=it',\n",
       "   'postag=PRP',\n",
       "   'BOS',\n",
       "   '+1:word.lower=is',\n",
       "   '+1:postag=VBZ'],\n",
       "  ['bias',\n",
       "   'word.lower=is',\n",
       "   'postag=VBZ',\n",
       "   '-1:word.lower=it',\n",
       "   '-1:postag=PRP',\n",
       "   '+1:word.lower=very',\n",
       "   '+1:postag=RB'],\n",
       "  ['bias',\n",
       "   'word.lower=very',\n",
       "   'postag=RB',\n",
       "   '-1:word.lower=is',\n",
       "   '-1:postag=VBZ',\n",
       "   '+1:word.lower=gud',\n",
       "   '+1:postag=JJ'],\n",
       "  ['bias',\n",
       "   'word.lower=gud',\n",
       "   'postag=JJ',\n",
       "   '-1:word.lower=very',\n",
       "   '-1:postag=RB',\n",
       "   '+1:word.lower=place',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=place',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=gud',\n",
       "   '-1:postag=JJ',\n",
       "   '+1:word.lower=to',\n",
       "   '+1:postag=TO'],\n",
       "  ['bias',\n",
       "   'word.lower=to',\n",
       "   'postag=TO',\n",
       "   '-1:word.lower=place',\n",
       "   '-1:postag=NN',\n",
       "   '+1:word.lower=get',\n",
       "   '+1:postag=VB'],\n",
       "  ['bias',\n",
       "   'word.lower=get',\n",
       "   'postag=VB',\n",
       "   '-1:word.lower=to',\n",
       "   '-1:postag=TO',\n",
       "   '+1:word.lower=food',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=food',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=get',\n",
       "   '-1:postag=VB',\n",
       "   '+1:word.lower=the',\n",
       "   '+1:postag=DT'],\n",
       "  ['bias',\n",
       "   'word.lower=the',\n",
       "   'postag=DT',\n",
       "   '-1:word.lower=food',\n",
       "   '-1:postag=NN',\n",
       "   '+1:word.lower=food',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=food',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=the',\n",
       "   '-1:postag=DT',\n",
       "   '+1:word.lower=is',\n",
       "   '+1:postag=VBZ'],\n",
       "  ['bias',\n",
       "   'word.lower=is',\n",
       "   'postag=VBZ',\n",
       "   '-1:word.lower=food',\n",
       "   '-1:postag=NN',\n",
       "   '+1:word.lower=too',\n",
       "   '+1:postag=RB'],\n",
       "  ['bias',\n",
       "   'word.lower=too',\n",
       "   'postag=RB',\n",
       "   '-1:word.lower=is',\n",
       "   '-1:postag=VBZ',\n",
       "   '+1:word.lower=gud',\n",
       "   '+1:postag=JJ'],\n",
       "  ['bias',\n",
       "   'word.lower=gud',\n",
       "   'postag=JJ',\n",
       "   '-1:word.lower=too',\n",
       "   '-1:postag=RB',\n",
       "   '+1:word.lower=and',\n",
       "   '+1:postag=CC'],\n",
       "  ['bias',\n",
       "   'word.lower=and',\n",
       "   'postag=CC',\n",
       "   '-1:word.lower=gud',\n",
       "   '-1:postag=JJ',\n",
       "   '+1:word.lower=mouth',\n",
       "   '+1:postag=JJ'],\n",
       "  ['bias',\n",
       "   'word.lower=mouth',\n",
       "   'postag=JJ',\n",
       "   '-1:word.lower=and',\n",
       "   '-1:postag=CC',\n",
       "   '+1:word.lower=water',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=water',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=mouth',\n",
       "   '-1:postag=JJ',\n",
       "   '+1:word.lower=fascinating',\n",
       "   '+1:postag=VBG'],\n",
       "  ['bias',\n",
       "   'word.lower=fascinating',\n",
       "   'postag=VBG',\n",
       "   '-1:word.lower=water',\n",
       "   '-1:postag=NN',\n",
       "   '+1:word.lower=quality',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=quality',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=fascinating',\n",
       "   '-1:postag=VBG',\n",
       "   '+1:word.lower=and',\n",
       "   '+1:postag=CC'],\n",
       "  ['bias',\n",
       "   'word.lower=and',\n",
       "   'postag=CC',\n",
       "   '-1:word.lower=quality',\n",
       "   '-1:postag=NN',\n",
       "   '+1:word.lower=quantity',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=quantity',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=and',\n",
       "   '-1:postag=CC',\n",
       "   '+1:word.lower=that',\n",
       "   '+1:postag=IN'],\n",
       "  ['bias',\n",
       "   'word.lower=that',\n",
       "   'postag=IN',\n",
       "   '-1:word.lower=quantity',\n",
       "   '-1:postag=NN',\n",
       "   '+1:word.lower=u',\n",
       "   '+1:postag=JJ'],\n",
       "  ['bias',\n",
       "   'word.lower=u',\n",
       "   'postag=JJ',\n",
       "   '-1:word.lower=that',\n",
       "   '-1:postag=IN',\n",
       "   '+1:word.lower=ever',\n",
       "   '+1:postag=RB'],\n",
       "  ['bias',\n",
       "   'word.lower=ever',\n",
       "   'postag=RB',\n",
       "   '-1:word.lower=u',\n",
       "   '-1:postag=JJ',\n",
       "   '+1:word.lower=going',\n",
       "   '+1:postag=VBG'],\n",
       "  ['bias',\n",
       "   'word.lower=going',\n",
       "   'postag=VBG',\n",
       "   '-1:word.lower=ever',\n",
       "   '-1:postag=RB',\n",
       "   '+1:word.lower=to',\n",
       "   '+1:postag=TO'],\n",
       "  ['bias',\n",
       "   'word.lower=to',\n",
       "   'postag=TO',\n",
       "   '-1:word.lower=going',\n",
       "   '-1:postag=VBG',\n",
       "   '+1:word.lower=get',\n",
       "   '+1:postag=VB'],\n",
       "  ['bias',\n",
       "   'word.lower=get',\n",
       "   'postag=VB',\n",
       "   '-1:word.lower=to',\n",
       "   '-1:postag=TO',\n",
       "   '+1:word.lower=awesome',\n",
       "   '+1:postag=JJ'],\n",
       "  ['bias',\n",
       "   'word.lower=awesome',\n",
       "   'postag=JJ',\n",
       "   '-1:word.lower=get',\n",
       "   '-1:postag=VB',\n",
       "   '+1:word.lower=food',\n",
       "   '+1:postag=NN'],\n",
       "  ['bias',\n",
       "   'word.lower=food',\n",
       "   'postag=NN',\n",
       "   '-1:word.lower=awesome',\n",
       "   '-1:postag=JJ',\n",
       "   '+1:word.lower=really',\n",
       "   '+1:postag=RB'],\n",
       "  ['bias',\n",
       "   'word.lower=really',\n",
       "   'postag=RB',\n",
       "   '-1:word.lower=food',\n",
       "   '-1:postag=NN',\n",
       "   '+1:word.lower=loved',\n",
       "   '+1:postag=VBD'],\n",
       "  ['bias',\n",
       "   'word.lower=loved',\n",
       "   'postag=VBD',\n",
       "   '-1:word.lower=really',\n",
       "   '-1:postag=RB',\n",
       "   '+1:word.lower=it',\n",
       "   '+1:postag=PRP'],\n",
       "  ['bias',\n",
       "   'word.lower=it',\n",
       "   'postag=PRP',\n",
       "   '-1:word.lower=loved',\n",
       "   '-1:postag=VBD',\n",
       "   'EOS']]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_test)\n",
    "display(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 58\n",
      "Seconds required: 0.002\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.010000\n",
      "num_memories: 6\n",
      "max_iterations: 200\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "***** Iteration #1 *****\n",
      "Loss: 4.843177\n",
      "Feature norm: 1.000000\n",
      "Error norm: 3.789306\n",
      "Active features: 58\n",
      "Line search trials: 1\n",
      "Line search step: 0.122006\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #2 *****\n",
      "Loss: 4.021040\n",
      "Feature norm: 0.991906\n",
      "Error norm: 2.580315\n",
      "Active features: 53\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.003\n",
      "\n",
      "***** Iteration #3 *****\n",
      "Loss: 2.831057\n",
      "Feature norm: 1.720565\n",
      "Error norm: 2.368734\n",
      "Active features: 58\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #4 *****\n",
      "Loss: 2.460305\n",
      "Feature norm: 1.975820\n",
      "Error norm: 1.296980\n",
      "Active features: 58\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #5 *****\n",
      "Loss: 2.166654\n",
      "Feature norm: 2.471035\n",
      "Error norm: 0.484500\n",
      "Active features: 58\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #6 *****\n",
      "Loss: 2.056379\n",
      "Feature norm: 2.809607\n",
      "Error norm: 0.364079\n",
      "Active features: 55\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #7 *****\n",
      "Loss: 1.946004\n",
      "Feature norm: 3.224822\n",
      "Error norm: 0.215522\n",
      "Active features: 28\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #8 *****\n",
      "Loss: 1.886131\n",
      "Feature norm: 3.445267\n",
      "Error norm: 0.177568\n",
      "Active features: 28\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #9 *****\n",
      "Loss: 1.806231\n",
      "Feature norm: 3.873937\n",
      "Error norm: 0.174917\n",
      "Active features: 18\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #10 *****\n",
      "Loss: 1.799183\n",
      "Feature norm: 3.792406\n",
      "Error norm: 0.424280\n",
      "Active features: 15\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #11 *****\n",
      "Loss: 1.744789\n",
      "Feature norm: 4.476199\n",
      "Error norm: 0.147316\n",
      "Active features: 13\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #12 *****\n",
      "Loss: 1.726649\n",
      "Feature norm: 4.381824\n",
      "Error norm: 0.063463\n",
      "Active features: 13\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #13 *****\n",
      "Loss: 1.713112\n",
      "Feature norm: 4.645256\n",
      "Error norm: 0.073104\n",
      "Active features: 13\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #14 *****\n",
      "Loss: 1.710593\n",
      "Feature norm: 4.593971\n",
      "Error norm: 0.171084\n",
      "Active features: 13\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #15 *****\n",
      "Loss: 1.708090\n",
      "Feature norm: 5.334563\n",
      "Error norm: 0.154907\n",
      "Active features: 12\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #16 *****\n",
      "Loss: 1.694443\n",
      "Feature norm: 5.227185\n",
      "Error norm: 0.077358\n",
      "Active features: 8\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #17 *****\n",
      "Loss: 1.693358\n",
      "Feature norm: 5.229017\n",
      "Error norm: 0.017577\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #18 *****\n",
      "Loss: 1.692936\n",
      "Feature norm: 5.138475\n",
      "Error norm: 0.024819\n",
      "Active features: 8\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #19 *****\n",
      "Loss: 1.692824\n",
      "Feature norm: 5.160744\n",
      "Error norm: 0.007490\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #20 *****\n",
      "Loss: 1.692769\n",
      "Feature norm: 5.145594\n",
      "Error norm: 0.004873\n",
      "Active features: 8\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #21 *****\n",
      "Loss: 1.692748\n",
      "Feature norm: 5.145118\n",
      "Error norm: 0.002813\n",
      "Active features: 8\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #22 *****\n",
      "Loss: 1.692738\n",
      "Feature norm: 5.144598\n",
      "Error norm: 0.002572\n",
      "Active features: 8\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #23 *****\n",
      "Loss: 1.692718\n",
      "Feature norm: 5.145140\n",
      "Error norm: 0.002661\n",
      "Active features: 8\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #24 *****\n",
      "Loss: 1.692715\n",
      "Feature norm: 5.144940\n",
      "Error norm: 0.004837\n",
      "Active features: 8\n",
      "Line search trials: 4\n",
      "Line search step: 0.125000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #25 *****\n",
      "Loss: 1.692706\n",
      "Feature norm: 5.144439\n",
      "Error norm: 0.005319\n",
      "Active features: 8\n",
      "Line search trials: 3\n",
      "Line search step: 0.250000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #26 *****\n",
      "Loss: 1.692699\n",
      "Feature norm: 5.144896\n",
      "Error norm: 0.005360\n",
      "Active features: 8\n",
      "Line search trials: 3\n",
      "Line search step: 0.250000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #27 *****\n",
      "Loss: 1.692690\n",
      "Feature norm: 5.144148\n",
      "Error norm: 0.003467\n",
      "Active features: 8\n",
      "Line search trials: 3\n",
      "Line search step: 0.250000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #28 *****\n",
      "Loss: 1.692685\n",
      "Feature norm: 5.145201\n",
      "Error norm: 0.003564\n",
      "Active features: 8\n",
      "Line search trials: 3\n",
      "Line search step: 0.250000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #29 *****\n",
      "Loss: 1.692682\n",
      "Feature norm: 5.144539\n",
      "Error norm: 0.002905\n",
      "Active features: 8\n",
      "Line search trials: 3\n",
      "Line search step: 0.250000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #30 *****\n",
      "Loss: 1.692678\n",
      "Feature norm: 5.144988\n",
      "Error norm: 0.002680\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #31 *****\n",
      "Loss: 1.692676\n",
      "Feature norm: 5.144113\n",
      "Error norm: 0.002548\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #32 *****\n",
      "Loss: 1.692674\n",
      "Feature norm: 5.144362\n",
      "Error norm: 0.002274\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #33 *****\n",
      "Loss: 1.692672\n",
      "Feature norm: 5.143537\n",
      "Error norm: 0.002045\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #34 *****\n",
      "Loss: 1.692671\n",
      "Feature norm: 5.143610\n",
      "Error norm: 0.002009\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #35 *****\n",
      "Loss: 1.692669\n",
      "Feature norm: 5.142687\n",
      "Error norm: 0.001595\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #36 *****\n",
      "Loss: 1.692669\n",
      "Feature norm: 5.142471\n",
      "Error norm: 0.001910\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #37 *****\n",
      "Loss: 1.692667\n",
      "Feature norm: 5.141474\n",
      "Error norm: 0.001166\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #38 *****\n",
      "Loss: 1.692667\n",
      "Feature norm: 5.141089\n",
      "Error norm: 0.001493\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #39 *****\n",
      "Loss: 1.692666\n",
      "Feature norm: 5.139828\n",
      "Error norm: 0.001213\n",
      "Active features: 8\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "L-BFGS terminated with the stopping criteria\n",
      "Total seconds required for training: 0.015\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 8 (58)\n",
      "Number of active attributes: 5 (51)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pycrfsuite\n",
    "trainer = pycrfsuite.Trainer(verbose=True)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "\n",
    "trainer.set_params({\n",
    "    'c1': 0.1, #L1 penalty\n",
    "    'c2': 0.01, #L2 penalty\n",
    "    'max_iterations': 200,\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "trainer.train('crf.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get y prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved (O)\n",
      "all (O)\n",
      "our (O)\n",
      "food (B)\n",
      ", (O)\n",
      "licensed (O)\n",
      ", (O)\n",
      "fast (O)\n",
      ", (O)\n",
      "efficient (O)\n",
      "service (B)\n",
      "[['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B'], ['O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O']]\n",
      "[['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B'], ['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('crf.model')\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved (O)\n",
      "all (O)\n",
      "our (O)\n",
      "food (B)\n",
      ", (O)\n",
      "licensed (O)\n",
      ", (O)\n",
      "fast (O)\n",
      ", (O)\n",
      "efficient (O)\n",
      "service (B)\n",
      "[['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B'], ['O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O']]\n",
      "[['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B'], ['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x, y in zip(y_pred[i], [x[1].split(\"=\")[1] for x in X_test[i]]):\n",
    "    print(\"%s (%s)\" % (y, x))\n",
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 0 2 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2]\n",
      "[2 2 2 0 2 2 2 2 2 2 0 2 2 2 2 0 2 2 0 2 0 2 2 2 2 2 0 2 0 2 0 2 2 2 2 2 2\n",
      " 2 0 2 2 2]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.44      1.00      0.62         4\n",
      "           O       1.00      0.87      0.93        38\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        42\n",
      "   macro avg       0.72      0.93      0.77        42\n",
      "weighted avg       0.95      0.88      0.90        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "labels = {\"B\": 0, \"I\": 1,\"O\":2}\n",
    "\n",
    "truth = np.array([labels[bio] for sentence in y_test for bio in sentence])\n",
    "prediction = np.array([labels[bio] for sentence in y_pred for bio in sentence])\n",
    "\n",
    "target_names = ['B', 'O']\n",
    "print(classification_report(truth, prediction, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
