{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1921"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "file = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "reviews = file['reviews'].values.tolist()\n",
    "foods = file['food'].values.tolist()\n",
    "services = file['service'].values.tolist()\n",
    "prices = file['price'].values.tolist()\n",
    "places = file['place'].values.tolist()\n",
    "\n",
    "tokenize_reviews = []\n",
    "for review in reviews:\n",
    "    word_tokens = word_tokenize(review)\n",
    "    tokenize_reviews.append(word_tokens)\n",
    "    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "filtered_reviews = []\n",
    "words = []\n",
    "\n",
    "def RemovePunctAndStopWords(tokens):\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "    filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "    #Remove the stopwords from filtered text\n",
    "    filtered_words = [word for word in filtered if word.lower() not in stoplist]\n",
    "    frequent_words=['the','and','of','this','am','etc','also','are','were','was','is']\n",
    "    filtered_words = [word for word in filtered_words if word.lower() not in frequent_words]\n",
    "    filtered_words=[word.lower() for word in filtered_words]\n",
    "    return filtered_words\n",
    "\n",
    "for review in tokenize_reviews:\n",
    "    review = RemovePunctAndStopWords(review)\n",
    "    filtered_reviews.append(review)\n",
    "\n",
    "def ngram_list(word_list, n):\n",
    "    all_ngrams = list(ngrams(word_list, n))\n",
    "    ngram_res = []\n",
    "    for ngram in all_ngrams:\n",
    "        ngram_res.append(ngram)\n",
    "    return ngram_res\n",
    "\n",
    "trigram_result = []\n",
    "# for i in range(len(filtered_reviews)):\n",
    "#     if (len(filtered_reviews[i]) > 2):\n",
    "#         trigram = ngram_list(filtered_reviews[i], 3)\n",
    "#         trigram_result.append(dict(Counter(trigram)))\n",
    "\n",
    "for i in range(len(filtered_reviews)):\n",
    "    unigram = ngram_list(filtered_reviews[i], 1)\n",
    "    trigram_result.append(dict(Counter(unigram)))\n",
    "#     bigram = {}\n",
    "#     if (len(filtered_reviews[i]) > 1):\n",
    "#         bigram = ngram_list(filtered_reviews[i], 2)\n",
    "#     trigram_result.append({**dict(Counter(unigram)), **dict(Counter(bigram))})\n",
    "\n",
    "trigram_set = set()\n",
    "for trigram in trigram_result:\n",
    "    for key in trigram:\n",
    "        trigram_set.add(key)\n",
    "        \n",
    "display(len(trigram_set))\n",
    "\n",
    "output_feature = pd.DataFrame(0, columns = trigram_set, index = [i for i in range(len(filtered_reviews))])\n",
    "for i, trigram in enumerate(trigram_result):\n",
    "    for k, v in iter(trigram.items()):\n",
    "        output_feature.at[i,k] += v\n",
    "\n",
    "output_feature.to_csv('output_encode.csv', index=False, header=False)\n",
    "\n",
    "food_df = pd.DataFrame(foods)\n",
    "service_df = pd.DataFrame(services)\n",
    "price_df = pd.DataFrame(prices)\n",
    "place_df = pd.DataFrame(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy food 0.6848552338530067\n",
      "Best Param food {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 0.25}\n",
      "[0.67032967 0.56043956 0.7032967  0.72527473 0.71111111 0.68888889\n",
      " 0.64444444 0.72727273 0.68181818 0.73863636]\n",
      "Best Accuracy price 0.9621380846325167\n",
      "Best Param price {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keziasuhendra/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 7 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95604396 0.96703297 0.94444444 0.95555556 0.95555556 0.95555556\n",
      " 0.95555556 0.96629213 0.96629213 0.98863636]\n",
      "Best Accuracy place 0.8496659242761693\n",
      "Best Param place {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 0.25}\n",
      "[0.82417582 0.84615385 0.87912088 0.82417582 0.86666667 0.75555556\n",
      " 0.87640449 0.86516854 0.86363636 0.86363636]\n",
      "Best Accuracy service 0.8674832962138085\n",
      "Best Param service {'n_estimators': 300, 'min_samples_leaf': 3, 'max_features': 0.25}\n",
      "[0.85869565 0.84615385 0.87777778 0.87777778 0.88888889 0.83146067\n",
      " 0.88764045 0.8988764  0.87640449 0.92134831]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def randomForest_fit(x_train, y_train, aspect, scoring = 'accuracy'):\n",
    "    randomForest = RandomForestClassifier(random_state=123)\n",
    "\n",
    "    hyperparam = {'min_samples_leaf': [3, 5, 7, 9, 13, 17, 21, 27, 33, 41, 50, 60, 80, 100],\n",
    "                  'max_features': ['sqrt', 'log2', 0.25, 0.5, 0.75], \n",
    "                  'n_estimators': [100, 200, 300, 500, 1000]}\n",
    "    \n",
    "    random_randomForest = RandomizedSearchCV(randomForest, param_distributions = hyperparam, cv = 5, \n",
    "                                           n_iter = 10, scoring = scoring, n_jobs=-1, random_state = 123)\n",
    "    random_randomForest.fit(x_train, y_train)\n",
    "    \n",
    "    print (\"Best Accuracy \" + aspect, random_randomForest.best_score_)\n",
    "    print (\"Best Param \" + aspect, random_randomForest.best_params_)\n",
    "    return random_randomForest\n",
    "\n",
    "# food\n",
    "best_randForest_food = randomForest_fit(output_feature, food_df.values.ravel(), \"food\")\n",
    "\n",
    "randForest_food = RandomForestClassifier(random_state=123, n_jobs = -1,\n",
    "                                   min_samples_leaf = best_randForest_food.best_params_.get('min_samples_leaf'),\n",
    "                                   max_features = best_randForest_food.best_params_.get('max_features'),\n",
    "                                   n_estimators = best_randForest_food.best_params_.get('n_estimators'))\n",
    "randForest_food.fit(output_feature, food_df.values.ravel())\n",
    "print(cross_val_score(randForest_food, output_feature, food_df.values.ravel(), cv=10))\n",
    "\n",
    "# price\n",
    "best_randForest_price = randomForest_fit(output_feature, price_df.values.ravel(), \"price\")\n",
    "\n",
    "randForest_price = RandomForestClassifier(random_state=123, n_jobs = -1,\n",
    "                                   min_samples_leaf = best_randForest_price.best_params_.get('min_samples_leaf'),\n",
    "                                   max_features = best_randForest_price.best_params_.get('max_features'),\n",
    "                                   n_estimators = best_randForest_price.best_params_.get('n_estimators'))\n",
    "randForest_price.fit(output_feature, price_df.values.ravel())\n",
    "print(cross_val_score(randForest_price, output_feature, price_df.values.ravel(), cv=10))\n",
    "\n",
    "# place\n",
    "best_randForest_place = randomForest_fit(output_feature, place_df.values.ravel(), \"place\")\n",
    "\n",
    "randForest_place = RandomForestClassifier(random_state=123, n_jobs = -1,\n",
    "                                   min_samples_leaf = best_randForest_place.best_params_.get('min_samples_leaf'),\n",
    "                                   max_features = best_randForest_place.best_params_.get('max_features'),\n",
    "                                   n_estimators = best_randForest_place.best_params_.get('n_estimators'))\n",
    "randForest_place.fit(output_feature, place_df.values.ravel())\n",
    "print(cross_val_score(randForest_place, output_feature, place_df.values.ravel(), cv=10))\n",
    "\n",
    "# service\n",
    "best_randForest_service = randomForest_fit(output_feature, service_df.values.ravel(), \"service\")\n",
    "\n",
    "randForest_service = RandomForestClassifier(random_state=123, n_jobs = -1,\n",
    "                                   min_samples_leaf = best_randForest_service.best_params_.get('min_samples_leaf'),\n",
    "                                   max_features = best_randForest_service.best_params_.get('max_features'),\n",
    "                                   n_estimators = best_randForest_service.best_params_.get('n_estimators'))\n",
    "randForest_service.fit(output_feature, service_df.values.ravel())\n",
    "print(cross_val_score(randForest_service, output_feature, service_df.values.ravel(), cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy 0.6614699331848553\n",
      "Best Param {'min_samples_leaf': 5, 'max_features': 0.5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=0.5, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def decTree_fit(x_train, y_train, scoring = 'accuracy'):\n",
    "    decTree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "    hyperparam = {'min_samples_leaf': [3, 5, 7, 9, 13, 17, 21, 27, 33, 41, 50, 60, 80, 100],\n",
    "                  'max_features': ['sqrt', 'log2', 0.25, 0.5, 0.75]}\n",
    "\n",
    "    random_decTree = RandomizedSearchCV(decTree, param_distributions = hyperparam, cv = 5,\n",
    "                                        n_iter = 15, scoring = scoring, n_jobs=-1, random_state = 123)\n",
    "    \n",
    "    random_decTree.fit(x_train, y_train)\n",
    "    \n",
    "    print (\"Best Accuracy\", random_decTree.best_score_)\n",
    "    print (\"Best Param\", random_decTree.best_params_)\n",
    "    \n",
    "    return random_decTree\n",
    "\n",
    "best_decTree = decTree_fit(output_feature, food_df.values.ravel())\n",
    "\n",
    "decTree = DecisionTreeClassifier(min_samples_leaf = best_decTree.best_params_.get('min_samples_leaf'),\n",
    "                                 max_features = best_decTree.best_params_.get('max_features'), random_state=123)\n",
    "decTree.fit(output_feature, food_df.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy 0.678173719376392\n",
      "Best Param {'n_estimators': 500, 'base_estimator__min_samples_leaf': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "         random_state=123, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def bagging_fit(x_train, y_train, scoring = 'accuracy'):\n",
    "    decTree = DecisionTreeClassifier(random_state=123)\n",
    "    \n",
    "    bagging = BaggingClassifier(base_estimator = decTree, random_state=123)\n",
    "    \n",
    "    hyperparam = {'base_estimator__min_samples_leaf': [3, 5, 7, 9, 13, 17, 21, 27, 33, 41, 50, 60, 80, 100],\n",
    "                  'n_estimators': [100, 200, 300, 500, 1000]}\n",
    "    # 'base_estimator__' sebelum 'min_samples_leaf' menandakan hyperparameter yang dicari ada di dalam base estimatornya\n",
    "    # dalam hal ini berarti decTree\n",
    "    # (min_samples_leaf ada di dalam decTree)\n",
    "    \n",
    "    random_bagging = RandomizedSearchCV(bagging, param_distributions = hyperparam, cv = 5, \n",
    "                                           n_iter = 10, scoring = scoring, n_jobs=-1, random_state = 123)\n",
    "    random_bagging.fit(x_train, y_train)\n",
    "    \n",
    "    print (\"Best Accuracy\", random_bagging.best_score_)\n",
    "    print (\"Best Param\", random_bagging.best_params_)\n",
    "    return random_bagging\n",
    "\n",
    "best_bagging = bagging_fit(output_feature, food_df.values.ravel())\n",
    "\n",
    "decTreeBag = DecisionTreeClassifier(min_samples_leaf = best_bagging.best_params_.get('base_estimator__min_samples_leaf'),\n",
    "                                    random_state=123)\n",
    "bagging = BaggingClassifier(base_estimator = decTreeBag, \n",
    "                            n_estimators = best_bagging.best_params_.get('n_estimators'),\n",
    "                            random_state=123, n_jobs=-1)\n",
    "bagging.fit(output_feature, food_df.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6043956  0.38461538 0.47252747 0.41758242 0.5        0.48888889\n",
      " 0.53333333 0.51136364 0.47727273 0.42045455]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(output_feature, food_df.values.ravel())\n",
    "print(cross_val_score(clf, output_feature, food_df.values.ravel(), cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy 0.5868596881959911\n",
      "Best Param {'C': 10}\n",
      "[0.58241758 0.58241758 0.58241758 0.58241758 0.58888889 0.58888889\n",
      " 0.58888889 0.59090909 0.59090909 0.59090909]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def rbfSVC_fit(x_train, y_train):\n",
    "    rbfSVC = SVC(kernel = 'rbf')\n",
    "\n",
    "    hyperparam = {'C': [1000, 333.33, 100, 33.33, 10, 3.33, 10, 3.33, 1, 0.33, 0.1, 0.033, 0.01, 0.0033, \n",
    "                        0.001, 0.00033, 0.0001]}\n",
    "\n",
    "    random_rbfSVC = RandomizedSearchCV(rbfSVC, param_distributions = hyperparam, cv = 5,\n",
    "                                    n_iter = 5, n_jobs=2, random_state = 123)\n",
    "    \n",
    "    random_rbfSVC.fit(x_train, y_train)\n",
    "    \n",
    "    print (\"Best Accuracy\", random_rbfSVC.score(x_train, y_train))\n",
    "    print (\"Best Param\", random_rbfSVC.best_params_)\n",
    "    \n",
    "    return random_rbfSVC \n",
    "\n",
    "best_rbfSVC = rbfSVC_fit(output_feature, food_df.values.ravel())\n",
    "\n",
    "RBF_SVC = SVC(kernel = 'rbf', C=best_rbfSVC.best_params_.get('C'))\n",
    "RBF_SVC.fit(output_feature, food_df.values.ravel())\n",
    "print(cross_val_score(RBF_SVC, output_feature, food_df.values.ravel(), cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy 0.6046770601336303\n",
      "Best Param {'n_estimators': 200, 'learning_rate': 1.0, 'base_estimator__min_samples_leaf': 41}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=41, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
       "            splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=200, random_state=123)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def adaBoost_fit(x_train, y_train, scoring = 'accuracy'):\n",
    "    decTree = DecisionTreeClassifier(random_state=123)\n",
    "    \n",
    "    adaBoost = AdaBoostClassifier(base_estimator = decTree, random_state=123)\n",
    "    \n",
    "    hyperparam = {'base_estimator__min_samples_leaf': [3, 5, 7, 9, 13, 17, 21, 27, 33, 41, 50, 60, 80, 100],\n",
    "                  'learning_rate':[1., .1, .01, .001],\n",
    "                  'n_estimators': [100, 200, 300]}\n",
    "    \n",
    "    random_adaBoost = RandomizedSearchCV(adaBoost, param_distributions = hyperparam, cv = 5, \n",
    "                                           n_iter = 1, scoring = scoring, n_jobs=-1, random_state = 123)\n",
    "    random_adaBoost.fit(x_train, y_train)\n",
    "    \n",
    "    print (\"Best Accuracy\", random_adaBoost.best_score_)\n",
    "    print (\"Best Param\", random_adaBoost.best_params_)\n",
    "    return random_adaBoost\n",
    "\n",
    "best_adaBoost = adaBoost_fit(output_feature, food_df.values.ravel())\n",
    "\n",
    "decTreeAdaBoost = DecisionTreeClassifier(min_samples_leaf = best_adaBoost.best_params_.get('base_estimator__min_samples_leaf'),\n",
    "                                    random_state=123)\n",
    "adaBoost = AdaBoostClassifier(base_estimator = decTreeAdaBoost, \n",
    "                            n_estimators = best_adaBoost.best_params_.get('n_estimators'),\n",
    "                            learning_rate = best_adaBoost.best_params_.get('learning_rate'),\n",
    "                            random_state=123)\n",
    "adaBoost.fit(output_feature, food_df.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(decTree,'decision_tree.pkl')\n",
    "joblib.dump(bagging,'bagging.pkl')\n",
    "joblib.dump(randForest_food,'random_forest_food.pkl')\n",
    "joblib.dump(randForest_price,'random_forest_price.pkl')\n",
    "joblib.dump(randForest_place,'random_forest_place.pkl')\n",
    "joblib.dump(randForest_service,'random_forest_service.pkl')\n",
    "joblib.dump(adaBoost,'adaptive_boosting.pkl')\n",
    "joblib.dump(clf, 'naive_bayes.pkl')\n",
    "joblib.dump(RBF_SVC, 'svm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01377953 0.98383176 0.00238871]]\n",
      "[[0.00160234 0.96747581 0.03092185]]\n",
      "[[5.27395223e-02 9.47233811e-01 2.66666667e-05]]\n",
      "[[0.570525   0.38879833 0.04067667]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "classifier = joblib.load('decision_tree.pkl')\n",
    "clf_load = joblib.load('naive_bayes.pkl')\n",
    "randForest_food = joblib.load('random_forest_food.pkl')\n",
    "randForest_price = joblib.load('random_forest_price.pkl')\n",
    "randForest_place = joblib.load('random_forest_place.pkl')\n",
    "randForest_service = joblib.load('random_forest_service.pkl')\n",
    "\n",
    "def predictData(classifier, datas, aspect):\n",
    "    tokenize_data = []\n",
    "    filtered_data = []\n",
    "    for review in datas:\n",
    "        data_tokens = word_tokenize(review)\n",
    "        tokenize_data.append(data_tokens)\n",
    "    \n",
    "    for review in tokenize_data:\n",
    "        review = RemovePunctAndStopWords(review)\n",
    "        filtered_data.append(review)\n",
    "    \n",
    "    data_result = []\n",
    "    for i in range(len(filtered_data)):\n",
    "        data = ngram_list(filtered_data[i], 1)\n",
    "        data_result.append(data)\n",
    "\n",
    "    feature = []\n",
    "    for trigram in trigram_set:\n",
    "        feature.append(data_result[0].count(trigram) if trigram in data_result[0] else 0)\n",
    "                \n",
    "    output_data = pd.DataFrame(feature)\n",
    "    output_data.to_csv('output_data.csv', index=False, header=False)\n",
    "    \n",
    "    result = classifier.predict(output_data.T)\n",
    "    result_proba = classifier.predict_proba(output_data.T)\n",
    "    print(result_proba)\n",
    "    if result > 0:\n",
    "        return('positive')\n",
    "    elif result < 0:\n",
    "        return('negative')\n",
    "    else:\n",
    "        return('neutral')\n",
    "\n",
    "text = [\"Call me nitpicky, but overcooked yolks pretty much kills the dish.\"]\n",
    "\n",
    "predictData(randForest_price, text, \"price\")\n",
    "predictData(randForest_place, text, \"place\")\n",
    "predictData(randForest_service, text, \"service\")\n",
    "predictData(randForest_food, text, \"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('call',), ('nitpicky',), ('overcooked',), ('yolks',), ('pretty',), ('much',), ('kills',), ('dish',)]]\n",
      "[[0.570525   0.38879833 0.04067667]]\n",
      "negative\n",
      "[[('call',), ('nitpicky',), ('overcooked',), ('yolks',), ('pretty',), ('much',), ('kills',), ('dish',)]]\n",
      "[[0.01377953 0.98383176 0.00238871]]\n",
      "neutral\n",
      "[[('call',), ('nitpicky',), ('overcooked',), ('yolks',), ('pretty',), ('much',), ('kills',), ('dish',)]]\n",
      "[[0.00160234 0.96747581 0.03092185]]\n",
      "neutral\n",
      "[[('call',), ('nitpicky',), ('overcooked',), ('yolks',), ('pretty',), ('much',), ('kills',), ('dish',)]]\n",
      "[[5.27395223e-02 9.47233811e-01 2.66666667e-05]]\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "def predictData(datas, aspect):\n",
    "    classifier = joblib.load('random_forest_food.pkl')\n",
    "    if (aspect == \"price\"):\n",
    "        classifier = joblib.load('random_forest_price.pkl')\n",
    "    if (aspect == \"place\"):\n",
    "        classifier = joblib.load('random_forest_place.pkl')\n",
    "    if (aspect == \"service\"):\n",
    "        classifier = joblib.load('random_forest_service.pkl')\n",
    "    \n",
    "    tokenize_data = []\n",
    "    filtered_data = []\n",
    "    for review in datas:\n",
    "        data_tokens = word_tokenize(review)\n",
    "        tokenize_data.append(data_tokens)\n",
    "    \n",
    "    for review in tokenize_data:\n",
    "        review = RemovePunctAndStopWords(review)\n",
    "        filtered_data.append(review)\n",
    "    \n",
    "    data_result = []\n",
    "    for i in range(len(filtered_data)):\n",
    "        data = ngram_list(filtered_data[i], 1)\n",
    "        data_result.append(data)\n",
    "        \n",
    "    print(data_result)\n",
    "\n",
    "    feature = []\n",
    "    for trigram in trigram_set:\n",
    "        feature.append(data_result[0].count(trigram) if trigram in data_result[0] else 0)\n",
    "                \n",
    "    output_data = pd.DataFrame(feature)\n",
    "    output_data.to_csv('output_data.csv', index=False, header=False)\n",
    "    \n",
    "    result = classifier.predict(output_data.T)\n",
    "    result_proba = classifier.predict_proba(output_data.T)\n",
    "    print(result_proba)\n",
    "    if result > 0:\n",
    "        return('positive')\n",
    "    elif result < 0:\n",
    "        return('negative')\n",
    "    else:\n",
    "        return('neutral')\n",
    "    \n",
    "    \n",
    "text = [\"Call me nitpicky, but overcooked yolks pretty much kills the dish.\"]\n",
    "\n",
    "print(predictData(text, \"food\"))\n",
    "print(predictData(text, \"price\"))\n",
    "print(predictData(text, \"place\"))\n",
    "print(predictData(text, \"service\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
